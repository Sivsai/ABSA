{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPSndSn0fNJ8WYxBN8bac/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sivsai/ABSA/blob/main/ABSA_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    DebertaForTokenClassification,\n",
        "    DebertaForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "id": "q5IBQB8BOHpI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NTWqRRLq0F9",
        "outputId": "fe26d2b0-4b28-4b3b-8419-5f1bf7bd73a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define helper functions (metrics, label creator)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "POl3a4phT-R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bio_labels(row):\n",
        "    sentence = row['Sentence']\n",
        "    aspect_terms = row['Aspect Term']\n",
        "    from_indices = row['from']\n",
        "    to_indices = row['to']\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    tokenized_output = tokenizer(\n",
        "        sentence,\n",
        "        return_offsets_mapping=True,\n",
        "        return_special_tokens_mask=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized_output['input_ids'])\n",
        "    offsets = tokenized_output['offset_mapping']\n",
        "    special_mask = tokenized_output['special_tokens_mask']\n",
        "\n",
        "    # Initialize labels with 'O'\n",
        "    labels = [label_map['O']] * len(tokens)\n",
        "\n",
        "\n",
        "     # Align BIO using character spans\n",
        "    for i in range(len(aspect_terms)):\n",
        "        start_char = from_indices[i]\n",
        "        end_char = to_indices[i]\n",
        "        is_first_token = True\n",
        "        for j, (offset_start, offset_end) in enumerate(offsets):\n",
        "            if special_mask[j] == 1:  # special tokens\n",
        "                continue\n",
        "            if offset_start >= start_char and offset_end <= end_char:\n",
        "                labels[j] = label_map['B-ASP'] if is_first_token else label_map['I-ASP']\n",
        "                is_first_token = False\n",
        "            elif offset_start < end_char and offset_end > start_char:\n",
        "                labels[j] = label_map['B-ASP'] if is_first_token else label_map['I-ASP']\n",
        "                is_first_token = False\n",
        "\n",
        "    # Ignore special tokens during loss\n",
        "    for j, is_special in enumerate(special_mask):\n",
        "        if is_special == 1:\n",
        "            labels[j] = -100\n",
        "\n",
        "    tokenized_output['labels'] = labels\n",
        "    return tokenized_output\n",
        "\n",
        "#Evalution or Compute metrices for extraction for model\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    true_predictions = [\n",
        "        [inv_label_map[p_] for (p_, l_) in zip(pred, lab) if l_ != -100]\n",
        "        for pred, lab in zip(preds, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [inv_label_map[l_] for (p_, l_) in zip(pred, lab) if l_ != -100]\n",
        "        for pred, lab in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    flat_preds = [x for seq in true_predictions for x in seq]\n",
        "    flat_labels = [x for seq in true_labels for x in seq]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(flat_labels, flat_preds),\n",
        "        \"f1\": f1_score(flat_labels, flat_preds, average='macro'),\n",
        "    }\n",
        "\n",
        "#Evalution for sentiment classfication model\n",
        "def compute_sentiment_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1\": f1_score(labels, predictions, average='macro')\n",
        "    }"
      ],
      "metadata": {
        "id": "044oOfrQThNa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define dataset classes"
      ],
      "metadata": {
        "id": "R6qg8Ta_RUIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset for Aspect Extraction\n",
        "class AspectDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(item['labels'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Dataset for Sentiment Classification\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, polarity_map, max_len=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.polarity_map = polarity_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.data[idx]['sentence']\n",
        "        aspect = self.data[idx]['aspect_term']\n",
        "        polarity = self.data[idx]['polarity']\n",
        "        text = f\"{aspect} [SEP] {sentence}\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.polarity_map[polarity], dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "kuYAvhstRTK9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define the training function (run_absa_training)"
      ],
      "metadata": {
        "id": "80aroEP0QCV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_absa_training(dataset_path, model_save_prefix):\n",
        "    \"\"\"\n",
        "    Loads a dataset, trains aspect and sentiment models, and saves them.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting training for {dataset_path} ---\")\n",
        "\n",
        "    # 1. Load and preprocess data\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    agg_df = df.groupby('id').agg({\n",
        "        'Sentence': 'first',\n",
        "        'Aspect Term': list,\n",
        "        'polarity': list,\n",
        "        'from': list,\n",
        "        'to': list\n",
        "    }).reset_index()\n",
        "\n",
        "    # --- Aspect Extraction Model Training ---\n",
        "    print(\"\\n--- Training Aspect Extraction Model ---\")\n",
        "    processed_data = agg_df.apply(create_bio_labels, axis=1).tolist()\n",
        "    train_data, test_data = train_test_split(processed_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_dataset = AspectDataset(train_data)\n",
        "    test_dataset = AspectDataset(test_data)\n",
        "\n",
        "    aspect_model = DebertaForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(label_map), ignore_mismatched_sizes=True)\n",
        "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "    aspect_output_dir = f'./results_{model_save_prefix}_aspect'\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=aspect_output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=aspect_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "    trainer.train()\n",
        "    trainer.save_model(aspect_output_dir)\n",
        "    tokenizer.save_pretrained(aspect_output_dir)\n",
        "\n",
        "    print(f\"Aspect model saved to {aspect_output_dir}\")\n",
        "\n",
        "    # --- Sentiment Classification Model Training ---\n",
        "    print(\"\\n--- Training Sentiment Classification Model ---\")\n",
        "    sentiment_data = []\n",
        "    for _, row in df.iterrows():\n",
        "        if pd.notna(row['Aspect Term']):\n",
        "            sentiment_data.append({\n",
        "                'sentence': row['Sentence'],\n",
        "                'aspect_term': row['Aspect Term'],\n",
        "                'polarity': row['polarity']\n",
        "            })\n",
        "\n",
        "    polarity_map = {p: i for i, p in enumerate(df['polarity'].unique())}\n",
        "\n",
        "    train_sentiment_data, test_sentiment_data = train_test_split(sentiment_data, test_size=0.2, random_state=42)\n",
        "    train_sentiment_dataset = SentimentDataset(train_sentiment_data, tokenizer, polarity_map=polarity_map)\n",
        "    test_sentiment_dataset = SentimentDataset(test_sentiment_data, tokenizer, polarity_map=polarity_map)\n",
        "\n",
        "    sentiment_model = DebertaForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME, num_labels=len(polarity_map),ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    sentiment_output_dir = f'./results_{model_save_prefix}_sentiment'\n",
        "    sentiment_training_args = TrainingArguments(\n",
        "        output_dir=sentiment_output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    sentiment_trainer = Trainer(\n",
        "        model=sentiment_model,\n",
        "        args=sentiment_training_args,\n",
        "        train_dataset=train_sentiment_dataset,\n",
        "        eval_dataset=test_sentiment_dataset,\n",
        "        compute_metrics=compute_sentiment_metrics\n",
        "    )\n",
        "    sentiment_trainer.train()\n",
        "    sentiment_trainer.save_model(sentiment_output_dir)\n",
        "    print(f\"Sentiment model saved to {sentiment_output_dir}\")\n",
        "\n",
        "    return aspect_output_dir, sentiment_output_dir, polarity_map"
      ],
      "metadata": {
        "id": "FpjEVa2rOizY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run training for one or multiple datasets"
      ],
      "metadata": {
        "id": "EiXFbnC0P47O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model name and tokenizer before running this cell\n",
        "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "laptopDataset = \"/content/drive/MyDrive/Colab_Data/ABSA_Datasets/Laptop_Train_v2.csv\"\n",
        "\n",
        "# Label map for aspect tagging (BIO labels)\n",
        "label_map = {'B-ASP': 0, 'I-ASP': 1, 'O': 2}\n",
        "inv_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "\n",
        "# Example: Train for the laptop dataset\n",
        "laptop_aspect_path, laptop_sentiment_path, laptop_polarity_map = run_absa_training(\n",
        "    dataset_path=laptopDataset,\n",
        "    model_save_prefix='laptop'\n",
        ")"
      ],
      "metadata": {
        "id": "6o_lM5g5TqCZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "18bb665f-6315-4439-d037-d9e67457ca2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting training for /content/drive/MyDrive/Colab_Data/ABSA_Datasets/Laptop_Train_v2.csv ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Aspect Extraction Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deberta-v2 to instantiate a model of type deberta. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of DebertaForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.10.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.11.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.2.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.3.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.4.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.5.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.5.attention.self.q_bias', 'deberta.encoder.layer.5.attention.self.v_bias', 'deberta.encoder.layer.6.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.6.attention.self.q_bias', 'deberta.encoder.layer.6.attention.self.v_bias', 'deberta.encoder.layer.7.attention.self.in_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.7.attention.self.q_bias', 'deberta.encoder.layer.7.attention.self.v_bias', 'deberta.encoder.layer.8.attention.self.in_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.8.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.8.attention.self.q_bias', 'deberta.encoder.layer.8.attention.self.v_bias', 'deberta.encoder.layer.9.attention.self.in_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.9.attention.self.q_bias', 'deberta.encoder.layer.9.attention.self.v_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DebertaForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized because the shapes did not match:\n",
            "- deberta.encoder.rel_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([1024, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3798945123.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='447' max='447' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [447/447 03:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.236000</td>\n",
              "      <td>0.203238</td>\n",
              "      <td>0.927471</td>\n",
              "      <td>0.725392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.171000</td>\n",
              "      <td>0.204957</td>\n",
              "      <td>0.930121</td>\n",
              "      <td>0.739528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.129100</td>\n",
              "      <td>0.195968</td>\n",
              "      <td>0.936744</td>\n",
              "      <td>0.756230</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aspect model saved to ./results_laptop_aspect\n",
            "\n",
            "--- Training Sentiment Classification Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deberta-v2 to instantiate a model of type deberta. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.10.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.11.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.2.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.3.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.4.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.5.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.5.attention.self.q_bias', 'deberta.encoder.layer.5.attention.self.v_bias', 'deberta.encoder.layer.6.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.6.attention.self.q_bias', 'deberta.encoder.layer.6.attention.self.v_bias', 'deberta.encoder.layer.7.attention.self.in_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.7.attention.self.q_bias', 'deberta.encoder.layer.7.attention.self.v_bias', 'deberta.encoder.layer.8.attention.self.in_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.8.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.8.attention.self.q_bias', 'deberta.encoder.layer.8.attention.self.v_bias', 'deberta.encoder.layer.9.attention.self.in_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.9.attention.self.q_bias', 'deberta.encoder.layer.9.attention.self.v_bias', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized because the shapes did not match:\n",
            "- deberta.encoder.rel_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([1024, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='708' max='708' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [708/708 05:53, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.104100</td>\n",
              "      <td>1.053978</td>\n",
              "      <td>0.563559</td>\n",
              "      <td>0.318100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.034100</td>\n",
              "      <td>0.985104</td>\n",
              "      <td>0.610169</td>\n",
              "      <td>0.344156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.829300</td>\n",
              "      <td>0.980444</td>\n",
              "      <td>0.616525</td>\n",
              "      <td>0.345320</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment model saved to ./results_laptop_sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RestaurantDataSet = \"/content/drive/MyDrive/Colab_Data/ABSA_Datasets/Restaurants_Train_v2.csv\"\n",
        "\n",
        "\n",
        "\n",
        "Restaurant_aspect_path, Restaurant_sentiment_path, Restaurant_polarity_map = run_absa_training(\n",
        "    dataset_path=RestaurantDataSet,\n",
        "    model_save_prefix='Restaurant'\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "klp0nlG0Pqam",
        "outputId": "fa1cdc95-bb9d-4118-cfae-88946246b13d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting training for /content/drive/MyDrive/Colab_Data/ABSA_Datasets/Restaurants_Train_v2.csv ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Aspect Extraction Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deberta-v2 to instantiate a model of type deberta. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of DebertaForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.10.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.11.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.2.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.3.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.4.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.5.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.5.attention.self.q_bias', 'deberta.encoder.layer.5.attention.self.v_bias', 'deberta.encoder.layer.6.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.6.attention.self.q_bias', 'deberta.encoder.layer.6.attention.self.v_bias', 'deberta.encoder.layer.7.attention.self.in_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.7.attention.self.q_bias', 'deberta.encoder.layer.7.attention.self.v_bias', 'deberta.encoder.layer.8.attention.self.in_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.8.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.8.attention.self.q_bias', 'deberta.encoder.layer.8.attention.self.v_bias', 'deberta.encoder.layer.9.attention.self.in_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.9.attention.self.q_bias', 'deberta.encoder.layer.9.attention.self.v_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DebertaForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized because the shapes did not match:\n",
            "- deberta.encoder.rel_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([1024, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3798945123.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='606' max='606' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [606/606 04:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.206200</td>\n",
              "      <td>0.222474</td>\n",
              "      <td>0.922489</td>\n",
              "      <td>0.723220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.198703</td>\n",
              "      <td>0.929137</td>\n",
              "      <td>0.752107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.118800</td>\n",
              "      <td>0.198244</td>\n",
              "      <td>0.933239</td>\n",
              "      <td>0.769472</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aspect model saved to ./results_Restaurant_aspect\n",
            "\n",
            "--- Training Sentiment Classification Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deberta-v2 to instantiate a model of type deberta. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.10.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.11.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.2.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.3.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.4.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.5.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.5.attention.self.q_bias', 'deberta.encoder.layer.5.attention.self.v_bias', 'deberta.encoder.layer.6.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.6.attention.self.q_bias', 'deberta.encoder.layer.6.attention.self.v_bias', 'deberta.encoder.layer.7.attention.self.in_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.7.attention.self.q_bias', 'deberta.encoder.layer.7.attention.self.v_bias', 'deberta.encoder.layer.8.attention.self.in_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.8.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.8.attention.self.q_bias', 'deberta.encoder.layer.8.attention.self.v_bias', 'deberta.encoder.layer.9.attention.self.in_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.9.attention.self.q_bias', 'deberta.encoder.layer.9.attention.self.v_bias', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized because the shapes did not match:\n",
            "- deberta.encoder.rel_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([1024, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1110' max='1110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1110/1110 08:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.936300</td>\n",
              "      <td>0.930430</td>\n",
              "      <td>0.572395</td>\n",
              "      <td>0.182014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.913400</td>\n",
              "      <td>1.083671</td>\n",
              "      <td>0.589986</td>\n",
              "      <td>0.223300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.848100</td>\n",
              "      <td>0.897311</td>\n",
              "      <td>0.650880</td>\n",
              "      <td>0.334683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment model saved to ./results_Restaurant_sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load saved models and test prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZT088Jrqo2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Laptop Models"
      ],
      "metadata": {
        "id": "iRnAD3keaCmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DebertaForTokenClassification, DebertaForSequenceClassification\n",
        "\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/Colab_Models/ABSA_Models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#Laptop Models\n",
        "laptop_aspect_model = DebertaForTokenClassification.from_pretrained(laptop_aspect_path)\n",
        "laptop_sentiment_model = DebertaForSequenceClassification.from_pretrained(laptop_sentiment_path)\n",
        "\n",
        "laptop_aspect_model.save_pretrained(f\"{save_dir}/laptop_aspect_model\")\n",
        "tokenizer.save_pretrained(f\"{save_dir}/laptop_aspect_model\")\n",
        "\n",
        "laptop_sentiment_model.save_pretrained(f\"{save_dir}/laptop_sentiment_model\")\n",
        "tokenizer.save_pretrained(f\"{save_dir}/laptop_sentiment_model\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWc8IVwHryoc",
        "outputId": "572da403-28af-482f-d54c-334ad7c738ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab_Models/ABSA_Models/laptop_sentiment_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab_Models/ABSA_Models/laptop_sentiment_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab_Models/ABSA_Models/laptop_sentiment_model/spm.model',\n",
              " '/content/drive/MyDrive/Colab_Models/ABSA_Models/laptop_sentiment_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/Colab_Models/ABSA_Models/laptop_sentiment_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Restaurant Models\n"
      ],
      "metadata": {
        "id": "II7uv7AuZ7Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Restaurant Models\n",
        "\n",
        "Restaurant_aspect_model = DebertaForTokenClassification.from_pretrained(Restaurant_aspect_path)\n",
        "Restaurant_sentiment_model = DebertaForSequenceClassification.from_pretrained(Restaurant_sentiment_path)\n",
        "\n",
        "Restaurant_aspect_model.save_pretrained(f\"{save_dir}/Restaurant_aspect_model\")\n",
        "tokenizer.save_pretrained(f\"{save_dir}/Restaurant_aspect_model\")\n",
        "\n",
        "Restaurant_sentiment_model.save_pretrained(f\"{save_dir}/Restaurant_sentiment_model\")\n",
        "tokenizer.save_pretrained(f\"{save_dir}/Restaurant_sentiment_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PqxtlKHZ4vq",
        "outputId": "5a1169ae-d7a3-443c-ed1c-d0ead8bfc606"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab_Models/ABSA_Models/Restaurant_sentiment_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab_Models/ABSA_Models/Restaurant_sentiment_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab_Models/ABSA_Models/Restaurant_sentiment_model/spm.model',\n",
              " '/content/drive/MyDrive/Colab_Models/ABSA_Models/Restaurant_sentiment_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/Colab_Models/ABSA_Models/Restaurant_sentiment_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict function"
      ],
      "metadata": {
        "id": "xseiNDbbZj9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict function\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def predict_absa(sentence, aspect_model, sentiment_model, tokenizer, inv_label_map, inv_polarity_map, max_len=128):\n",
        "    aspect_model.eval()\n",
        "    sentiment_model.eval()\n",
        "\n",
        "    # ========== 1 ASPECT EXTRACTION ==========\n",
        "    inputs = tokenizer(\n",
        "        sentence,\n",
        "        return_tensors='pt',\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_len\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = aspect_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=2).squeeze().tolist()\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
        "    labels = [inv_label_map[p] for p in preds]\n",
        "\n",
        "    # Extract aspect terms from BIO tags\n",
        "    aspects = []\n",
        "    current_aspect = []\n",
        "    for token, label in zip(tokens, labels):\n",
        "        if label == 'B-ASP':\n",
        "            if current_aspect:\n",
        "                aspects.append(' '.join(current_aspect))\n",
        "                current_aspect = []\n",
        "            current_aspect.append(token)\n",
        "        elif label == 'I-ASP' and current_aspect:\n",
        "            current_aspect.append(token)\n",
        "        else:\n",
        "            if current_aspect:\n",
        "                aspects.append(' '.join(current_aspect))\n",
        "                current_aspect = []\n",
        "    if current_aspect:\n",
        "        aspects.append(' '.join(current_aspect))\n",
        "\n",
        "    # Clean up subword tokens like \"##ing\", \"##ly\"\n",
        "    clean_aspects = []\n",
        "    for asp in aspects:\n",
        "        asp = asp.replace(\"##\", \"\")\n",
        "        asp = asp.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
        "        if asp:\n",
        "            clean_aspects.append(asp)\n",
        "\n",
        "    if not clean_aspects:\n",
        "        return {\"aspects\": [], \"sentiments\": []}\n",
        "\n",
        "    # ========== 2 SENTIMENT CLASSIFICATION ==========\n",
        "    sentiments = []\n",
        "    for aspect in clean_aspects:\n",
        "        combined_text = f\"{aspect} [SEP] {sentence}\"\n",
        "        enc = tokenizer(\n",
        "            combined_text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_len\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = sentiment_model(**enc)\n",
        "            pred = torch.argmax(output.logits, dim=1).item()\n",
        "\n",
        "        sentiments.append(inv_polarity_map[pred])\n",
        "\n",
        "    return {\"aspects\": clean_aspects, \"sentiments\": sentiments}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "# Load the fine-tuned laptop models\n",
        "\n",
        "laptop_aspect_model = DebertaForTokenClassification.from_pretrained(f\"{save_dir}/laptop_aspect_model\")\n",
        "laptop_sentiment_model = DebertaForSequenceClassification.from_pretrained(f\"{save_dir}/laptop_sentiment_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{save_dir}/laptop_aspect_model\")\n",
        "\n",
        "inv_laptop_polarity_map = {v: k for k, v in laptop_polarity_map.items()}\n",
        "\n",
        "# Test on a sample review\n",
        "laptop_review = \"The keyboard is great but the battery dies quickly.\"\n",
        "results = predict_absa(\n",
        "    laptop_review,\n",
        "    laptop_aspect_model,\n",
        "    laptop_sentiment_model,\n",
        "    tokenizer,\n",
        "    inv_label_map,\n",
        "    inv_laptop_polarity_map\n",
        ")\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0bBddFhpfLu",
        "outputId": "6d11c27c-194f-4e6f-c0f1-321317783b93"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'aspects': ['keyboard', 'battery'], 'sentiments': ['positive', 'positive']}\n"
          ]
        }
      ]
    }
  ]
}